# Reading: Summary and Highlights

Congratulations! You have completed this lesson. At this point in the course, you know that: 

* Fine-tuning in machine learning is the process of adapting a pretrained model for specific tasks or use cases. During fine-tuning, the collate function tokenizes the dataset, the transformer-based model class defines classification in PyTorch, the forward method applies embeddings to the input, and the train_model function trains a transformer model.
* Fine-tuning enhances efficiency and saves time and computational resources compared to training models from scratch. It helps to transfer learning, time and resource efficiency, tailored responses, and task-specific adaptation.
* HuggingFace is an open-source machine learning or ML platform with a built-in transformers library for natural language processing (or NLP) applications. Its built-in datasets can be loaded using the load_dataset function.
* The differences between Hugging Face and PyTorch are tabulated below:

| **Hugging Face** | **PyTorch** |
| ---------------- | ----------- |
| Hugging Face is a platform and community dedicated to machine learning (ML) and data science, also known as GitHub of machine learning." | PyTorch is a software-based open-source deep learning framework that supports a wide variety of neural network architectures. |
| The Transformers library is the most popular feature of Hugging Face. | The dynamic computation graph is the most popular feature of PyTorch. |

